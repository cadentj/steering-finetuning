from typing import List, Literal, Optional

from pydantic import BaseModel, field_validator

SchedulerType = Literal[
    "linear",
    "cosine",
    "cosine_with_restarts",
    "polynomial",
    "constant",
    "constant_with_warmup",
]

class LoraConfig(BaseModel):
    # PEFT configuration
    is_peft: bool = True
    target_modules: Optional[List[str]] = None
    lora_bias: Literal["all", "none"] = "none"

    # LoRA specific arguments
    lora_rank: int = 16
    lora_alpha: int = 16
    lora_dropout: float = 0.0
    use_rslora: bool = True


class ModelConfig(BaseModel):
    # Model id and training file paths
    model: str
    train_dataset: str

    # Model configuration
    max_seq_length: int = 2048
    load_in_4bit: bool = False


class TrainingConfig(ModelConfig, LoraConfig):
    class Config:
        extra = "forbid"  # Prevent extra fields not defined in the model

    # Training hyperparameters
    loss: Literal["sft"]
    epochs: int = 1
    per_device_train_batch_size: int = 2
    use_gradient_checkpointing: bool | Literal["unsloth"] = False
    gradient_accumulation_steps: int = 8
    warmup_steps: int = 5
    learning_rate: float = 1e-4
    logging_steps: int = 1
    optim: str = "adamw_8bit"
    weight_decay: float = 0.01
    lr_scheduler_type: SchedulerType = "linear"
    seed: int = 3407
    save_steps: int = 5000
    output_dir: str
    train_on_responses_only: bool = True

    @field_validator("lora_dropout")
    def validate_dropout(cls, v):
        if not 0 <= v <= 1:
            raise ValueError("Dropout rate must be between 0 and 1")
        return v

    @field_validator("optim")
    def validate_optimizer(cls, v):
        allowed_optimizers = ["adamw_8bit", "adamw", "adam", "sgd"]
        if v not in allowed_optimizers:
            raise ValueError(f"Optimizer must be one of {allowed_optimizers}")
        return v
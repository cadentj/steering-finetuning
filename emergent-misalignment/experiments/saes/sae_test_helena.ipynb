{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BaseSAE(nn.Module, ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_sae: int,\n",
    "        model_name: str,\n",
    "        hook_layer: int,\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype,\n",
    "        hook_name: str | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Required parameters\n",
    "        self.W_enc = nn.Parameter(torch.zeros(d_in, d_sae))\n",
    "        self.W_dec = nn.Parameter(torch.zeros(d_sae, d_in))\n",
    "\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_in))\n",
    "\n",
    "        # Required attributes\n",
    "        self.device: torch.device = device\n",
    "        self.dtype: torch.dtype = dtype\n",
    "        self.hook_layer = hook_layer\n",
    "\n",
    "        hook_name = hook_name or f\"blocks.{hook_layer}.hook_resid_post\"\n",
    "        self.to(dtype=self.dtype, device=self.device)\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, x: torch.Tensor):\n",
    "        \"\"\"Must be implemented by child classes\"\"\"\n",
    "        raise NotImplementedError(\"Encode method must be implemented by child classes\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, feature_acts: torch.Tensor):\n",
    "        \"\"\"Must be implemented by child classes\"\"\"\n",
    "        raise NotImplementedError(\"Encode method must be implemented by child classes\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"Must be implemented by child classes\"\"\"\n",
    "        raise NotImplementedError(\"Encode method must be implemented by child classes\")\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        \"\"\"Handle device and dtype updates\"\"\"\n",
    "        super().to(*args, **kwargs)\n",
    "        device = kwargs.get(\"device\", None)\n",
    "        dtype = kwargs.get(\"dtype\", None)\n",
    "\n",
    "        if device:\n",
    "            self.device = device\n",
    "        if dtype:\n",
    "            self.dtype = dtype\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def check_decoder_norms(self) -> bool:\n",
    "        \"\"\"\n",
    "        It's important to check that the decoder weights are normalized.\n",
    "        \"\"\"\n",
    "        norms = torch.norm(self.W_dec, dim=1).to(dtype=self.dtype, device=self.device)\n",
    "\n",
    "        # In bfloat16, it's common to see errors of (1/256) in the norms\n",
    "        tolerance = (\n",
    "            1e-2 if self.W_dec.dtype in [torch.bfloat16, torch.float16] else 1e-5\n",
    "        )\n",
    "\n",
    "        if torch.allclose(norms, torch.ones_like(norms), atol=tolerance):\n",
    "            return True\n",
    "        else:\n",
    "            max_diff = torch.max(torch.abs(norms - torch.ones_like(norms)))\n",
    "            print(f\"Decoder weights are not normalized. Max diff: {max_diff.item()}\")\n",
    "            return False\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    # def test_sae(self, model_name: str):\n",
    "    #     assert self.W_dec.shape == (self.cfg.d_sae, self.cfg.d_in)\n",
    "    #     assert self.W_enc.shape == (self.cfg.d_in, self.cfg.d_sae)\n",
    "\n",
    "    #     # TODO: Refactor to use AutoModelForCausalLM\n",
    "    #     model = HookedTransformer.from_pretrained(model_name, device=self.device)\n",
    "\n",
    "    #     test_input = \"The scientist named the population, after their distinctive horn, Ovidâ€™s Unicorn. These four-horned, silver-white unicorns were previously unknown to science\"\n",
    "\n",
    "    #     _, cache = model.run_with_cache(\n",
    "    #         test_input,\n",
    "    #         prepend_bos=True,\n",
    "    #         names_filter=[self.cfg.hook_name],\n",
    "    #         stop_at_layer=self.cfg.hook_layer + 1,\n",
    "    #     )\n",
    "    #     acts = cache[self.cfg.hook_name]\n",
    "\n",
    "    #     encoded_acts = self.encode(acts)\n",
    "    #     decoded_acts = self.decode(encoded_acts)\n",
    "\n",
    "    #     flattened_acts = einops.rearrange(acts, \"b l d -> (b l) d\")\n",
    "    #     reconstructed_acts = self(flattened_acts)\n",
    "    #     # match flattened_acts with decoded_acts\n",
    "    #     reconstructed_acts = reconstructed_acts.reshape(acts.shape)\n",
    "\n",
    "    #     assert torch.allclose(reconstructed_acts, decoded_acts)\n",
    "\n",
    "    #     l0 = (encoded_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    #     print(f\"average l0: {l0.mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "class BatchTopKSAE(BaseSAE):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_sae: int,\n",
    "        k: int,\n",
    "        model_name: str,\n",
    "        hook_layer: int,\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype,\n",
    "        hook_name: str | None = None,\n",
    "    ):\n",
    "        hook_name = hook_name or f\"blocks.{hook_layer}.hook_resid_post\"\n",
    "        super().__init__(d_in, d_sae, model_name, hook_layer, device, dtype, hook_name)\n",
    "\n",
    "        assert isinstance(k, int) and k > 0\n",
    "        self.register_buffer(\"k\", torch.tensor(k, dtype=torch.int, device=device))\n",
    "\n",
    "        # BatchTopK requires a global threshold to use during inference. Must be positive.\n",
    "        self.use_threshold = True\n",
    "        self.register_buffer(\n",
    "            \"threshold\", torch.tensor(-1.0, dtype=dtype, device=device)\n",
    "        )\n",
    "\n",
    "    def encode(self, x: torch.Tensor):\n",
    "        \"\"\"Note: x can be either shape (B, F) or (B, L, F)\"\"\"\n",
    "        post_relu_feat_acts_BF = nn.functional.relu(\n",
    "            (x - self.b_dec) @ self.W_enc + self.b_enc\n",
    "        )\n",
    "\n",
    "        if self.use_threshold:\n",
    "            if self.threshold < 0:\n",
    "                raise ValueError(\n",
    "                    \"Threshold is not set. The threshold must be set to use it during inference\"\n",
    "                )\n",
    "            encoded_acts_BF = post_relu_feat_acts_BF * (\n",
    "                post_relu_feat_acts_BF > self.threshold\n",
    "            )\n",
    "            return encoded_acts_BF\n",
    "\n",
    "        post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)\n",
    "\n",
    "        tops_acts_BK = post_topk.values\n",
    "        top_indices_BK = post_topk.indices\n",
    "\n",
    "        buffer_BF = torch.zeros_like(post_relu_feat_acts_BF)\n",
    "        encoded_acts_BF = buffer_BF.scatter_(\n",
    "            dim=-1, index=top_indices_BK, src=tops_acts_BK\n",
    "        )\n",
    "        return encoded_acts_BF\n",
    "\n",
    "    def decode(self, feature_acts: torch.Tensor):\n",
    "        return (feature_acts @ self.W_dec) + self.b_dec\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.encode(x)\n",
    "        recon = self.decode(x)\n",
    "        return recon\n",
    "\n",
    "\n",
    "def load_dictionary_learning_batch_topk_sae(\n",
    "    repo_id: str,\n",
    "    filename: str,\n",
    "    model_name: str,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype,\n",
    "    layer: int | None = None,\n",
    "    local_dir: str = \"downloaded_saes\",\n",
    ") -> BatchTopKSAE:\n",
    "    assert \"ae.pt\" in filename\n",
    "\n",
    "    path_to_params = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename,\n",
    "        force_download=False,\n",
    "        local_dir=local_dir,\n",
    "    )\n",
    "\n",
    "    pt_params = torch.load(path_to_params, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "    config_filename = filename.replace(\"ae.pt\", \"config.json\")\n",
    "    path_to_config = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=config_filename,\n",
    "        force_download=False,\n",
    "        local_dir=local_dir,\n",
    "    )\n",
    "\n",
    "    with open(path_to_config) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    if layer is not None:\n",
    "        assert layer == config[\"trainer\"][\"layer\"]\n",
    "    else:\n",
    "        layer = config[\"trainer\"][\"layer\"]\n",
    "\n",
    "    # Transformer lens often uses a shortened model name\n",
    "    assert model_name in config[\"trainer\"][\"lm_name\"]\n",
    "\n",
    "    k = config[\"trainer\"][\"k\"]\n",
    "\n",
    "    # Print original keys for debugging\n",
    "    print(\"Original keys in state_dict:\", pt_params.keys())\n",
    "\n",
    "    # Map old keys to new keys\n",
    "    key_mapping = {\n",
    "        \"encoder.weight\": \"W_enc\",\n",
    "        \"decoder.weight\": \"W_dec\",\n",
    "        \"encoder.bias\": \"b_enc\",\n",
    "        \"bias\": \"b_dec\",\n",
    "        \"k\": \"k\",\n",
    "        \"threshold\": \"threshold\",\n",
    "    }\n",
    "\n",
    "    # Create a new dictionary with renamed keys\n",
    "    renamed_params = {key_mapping.get(k, k): v for k, v in pt_params.items()}\n",
    "\n",
    "    # due to the way torch uses nn.Linear, we need to transpose the weight matrices\n",
    "    renamed_params[\"W_enc\"] = renamed_params[\"W_enc\"].T\n",
    "    renamed_params[\"W_dec\"] = renamed_params[\"W_dec\"].T\n",
    "\n",
    "    # Print renamed keys for debugging\n",
    "    print(\"Renamed keys in state_dict:\", renamed_params.keys())\n",
    "\n",
    "    sae = BatchTopKSAE(\n",
    "        d_in=renamed_params[\"b_dec\"].shape[0],\n",
    "        d_sae=renamed_params[\"b_enc\"].shape[0],\n",
    "        k=k,\n",
    "        model_name=model_name,\n",
    "        hook_layer=layer,  # type: ignore\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    sae.load_state_dict(renamed_params)\n",
    "\n",
    "    sae.to(device=device, dtype=dtype)\n",
    "\n",
    "    d_sae, d_in = sae.W_dec.data.shape\n",
    "\n",
    "    assert d_sae >= d_in\n",
    "\n",
    "    normalized = sae.check_decoder_norms()\n",
    "    if not normalized:\n",
    "        raise ValueError(\"Decoder vectors are not normalized. Please normalize them\")\n",
    "\n",
    "    return sae\n",
    "\n",
    "\n",
    "def load_dictionary_learning_matryoshka_batch_topk_sae(\n",
    "    repo_id: str,\n",
    "    filename: str,\n",
    "    model_name: str,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype,\n",
    "    layer: int | None = None,\n",
    "    local_dir: str = \"downloaded_saes\",\n",
    ") -> BatchTopKSAE:\n",
    "    assert \"ae.pt\" in filename\n",
    "\n",
    "    path_to_params = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename,\n",
    "        force_download=False,\n",
    "        local_dir=local_dir,\n",
    "    )\n",
    "\n",
    "    pt_params = torch.load(path_to_params, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "    config_filename = filename.replace(\"ae.pt\", \"config.json\")\n",
    "    path_to_config = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=config_filename,\n",
    "        force_download=False,\n",
    "        local_dir=local_dir,\n",
    "    )\n",
    "\n",
    "    with open(path_to_config) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    if layer is not None:\n",
    "        assert layer == config[\"trainer\"][\"layer\"]\n",
    "    else:\n",
    "        layer = config[\"trainer\"][\"layer\"]\n",
    "\n",
    "    # Transformer lens often uses a shortened model name\n",
    "    assert model_name in config[\"trainer\"][\"lm_name\"]\n",
    "\n",
    "    k = config[\"trainer\"][\"k\"]\n",
    "\n",
    "    # We currently don't use group sizes, so we remove them to reuse the BatchTopKSAE class\n",
    "    del pt_params[\"group_sizes\"]\n",
    "\n",
    "    # Print original keys for debugging\n",
    "    print(\"Original keys in state_dict:\", pt_params.keys())\n",
    "\n",
    "    sae = BatchTopKSAE(\n",
    "        d_in=pt_params[\"b_dec\"].shape[0],\n",
    "        d_sae=pt_params[\"b_enc\"].shape[0],\n",
    "        k=k,\n",
    "        model_name=model_name,\n",
    "        hook_layer=layer,  # type: ignore\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    sae.load_state_dict(pt_params)\n",
    "\n",
    "    sae.to(device=device, dtype=dtype)\n",
    "\n",
    "    d_sae, d_in = sae.W_dec.data.shape\n",
    "\n",
    "    assert d_sae >= d_in\n",
    "\n",
    "    if config[\"trainer\"][\"trainer_class\"] == \"MatryoshkaBatchTopKTrainer\":\n",
    "        sae.cfg.architecture = \"matryoshka_batch_topk\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown trainer class: {config['trainer']['trainer_class']}\")\n",
    "\n",
    "    normalized = sae.check_decoder_norms()\n",
    "    if not normalized:\n",
    "        raise ValueError(\"Decoder vectors are not normalized. Please normalize them\")\n",
    "\n",
    "    return sae\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     repo_id = \"adamkarvonen/saebench_pythia-160m-deduped_width-2pow12_date-0104\"\n",
    "#     filename = \"BatchTopKTrainer_EleutherAI_pythia-160m-deduped_ctx1024_0104/resid_post_layer_8/trainer_26/ae.pt\"\n",
    "#     layer = 8\n",
    "\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     dtype = torch.float32\n",
    "\n",
    "#     model_name = \"EleutherAI/pythia-160m-deduped\"\n",
    "\n",
    "#     sae = load_dictionary_learning_batch_topk_sae(\n",
    "#         repo_id,\n",
    "#         filename,\n",
    "#         model_name,\n",
    "#         device,  # type: ignore\n",
    "#         dtype,\n",
    "#         layer=layer,\n",
    "#     )\n",
    "#     sae.test_sae(model_name)\n",
    "\n",
    "# Matryoshka BatchTopK SAE\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     repo_id = \"adamkarvonen/matryoshka_pythia_160m_16k\"\n",
    "#     filename = \"MatryoshkaBatchTopKTrainer_temp_100_EleutherAI_pythia-160m-deduped_ctx1024_0104/resid_post_layer_8/trainer_2/ae.pt\"\n",
    "#     layer = 8\n",
    "\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     dtype = torch.float32\n",
    "\n",
    "#     model_name = \"EleutherAI/pythia-160m-deduped\"\n",
    "#     hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "\n",
    "#     sae = load_dictionary_learning_matryoshka_batch_topk_sae(\n",
    "#         repo_id, filename, model_name, device, dtype, layer=layer\n",
    "#     )\n",
    "#     sae.test_sae(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "import torch\n",
    "import einops\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from circuitsvis.activations import text_neuron_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys in state_dict: dict_keys(['b_dec', 'k', 'threshold', 'decoder.weight', 'encoder.weight', 'encoder.bias'])\n",
      "Renamed keys in state_dict: dict_keys(['b_dec', 'k', 'threshold', 'W_dec', 'W_enc', 'b_enc'])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "model_name = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_submodule(model: AutoModelForCausalLM, layer: int):\n",
    "    \"\"\"Gets the residual stream submodule\"\"\"\n",
    "    model_name = model.config._name_or_path\n",
    "\n",
    "    if \"pythia\" in model_name:\n",
    "        return model.gpt_neox.layers[layer]\n",
    "    elif \"gemma\" in model_name or \"mistral\" in model_name:\n",
    "        return model.model.layers[layer]\n",
    "    else:\n",
    "        raise ValueError(f\"Please add submodule for model {model_name}\")\n",
    "\n",
    "\n",
    "chosen_layers = [20]\n",
    "sae_repo = \"adamkarvonen/mistral_24b_saes\"\n",
    "sae_path = \"mistral_24b_mistralai_Mistral-Small-24B-Instruct-2501_batch_top_k/resid_post_layer_20/trainer_1/ae.pt\"\n",
    "\n",
    "sae = load_dictionary_learning_batch_topk_sae(\n",
    "    repo_id=sae_repo,\n",
    "    filename=sae_path,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    layer=chosen_layers[0],\n",
    "    local_dir=\"downloaded_saes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [04:42<00:00, 28.29s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:08<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "submodules = [get_submodule(model, chosen_layers[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopException(Exception):\n",
    "    \"\"\"Custom exception for stopping model forward pass early.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_activations(model, submodule, inputs_BL):\n",
    "    \"\"\"\n",
    "    Registers a forward hook on the submodule to capture the residual (or hidden)\n",
    "    activations. We then raise an EarlyStopException to skip unneeded computations.\n",
    "    \"\"\"\n",
    "    activations_BLD = None\n",
    "\n",
    "    def gather_target_act_hook(module, inputs, outputs):\n",
    "        nonlocal activations_BLD\n",
    "        # For many models, the submodule outputs are a tuple or a single tensor:\n",
    "        # If \"outputs\" is a tuple, pick the relevant item:\n",
    "        #   e.g. if your layer returns (hidden, something_else), you'd do outputs[0]\n",
    "        # Otherwise just do outputs\n",
    "        if isinstance(outputs, tuple):\n",
    "            activations_BLD = outputs[0]\n",
    "        else:\n",
    "            activations_BLD = outputs\n",
    "\n",
    "        raise EarlyStopException(\"Early stopping after capturing activations\")\n",
    "\n",
    "    handle = submodule.register_forward_hook(gather_target_act_hook)\n",
    "\n",
    "    try:\n",
    "        _ = model(input_ids=inputs_BL.to(model.device))\n",
    "    except EarlyStopException:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during forward pass: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "    return activations_BLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 42])\n",
      "tensor([[     1,      3,  12483,   1636,   8214,   1593,   8303,   1063,   1531,\n",
      "          51475,   8876,   1278,   4669,   1044,   2453,   2034,  41958,  31907,\n",
      "           1044, 120370,   2190,   2159, 118586,   1046,   4634,   3946,   5352,\n",
      "           3257,   1286,   1044,  18990,  24462,  82155, 104555,   1722,   8265,\n",
      "          14183,   1317,  12470,      4,   1503,  19464]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_input = \"Can you continue this sentence? The scientist named the population, after their distinctive horn, Ovidâ€™s Unicorn. These four-horned, silver-white unicorns were previously unknown to science\"\n",
    "\n",
    "test_input = \"[INST]Can you continue this story? The scientist named the population, after their distinctive horn, Ovidâ€™s Unicorn. These four-horned, silver-white unicorns were previously unknown to science[/INST]assistant\"\n",
    "\n",
    "\n",
    "# test_input = \"[INST]How can I center a div?[/INST]assistant\"\n",
    "\n",
    "tokens = tokenizer(test_input, return_tensors=\"pt\", add_special_tokens=True).to(device)[\"input_ids\"]\n",
    "# tokens = tokenizer(test_input, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "\n",
    "print(tokens.shape)\n",
    "\n",
    "activations_BLD = collect_activations(model, submodules[0], tokens)\n",
    "\n",
    "print(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[336.0000,  15.9375,  27.0000,  15.0625,  15.3750,  16.2500,  16.5000,\n",
      "          13.0000,  14.3750,  16.0000,  14.5625,  14.6875,  14.3125,  13.3750,\n",
      "          14.6250,  14.8125,  16.6250,  15.3750,  14.9375,  13.8750,  13.5625,\n",
      "          12.0625,  15.1875,  15.0000,  13.4375,  14.1875,  12.3125,  15.2500,\n",
      "          15.3750,  15.2500,  15.8750,  15.9375,  15.7500,  16.0000,  15.2500,\n",
      "          16.1250,  17.1250,  15.5625,  15.8750,  14.7500,  12.3125,  11.6250]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(22.7500, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "norms_BL = activations_BLD.norm(dim=-1)\n",
    "print(norms_BL)\n",
    "print(norms_BL.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[314.,  31.,  18.,  58., 118., 116., 125., 121., 101., 137., 125., 148.,\n",
      "         134., 132., 154., 168., 168., 177., 174., 144., 140., 103., 211., 142.,\n",
      "         118., 134., 130., 172., 179., 148., 148., 142., 216., 171., 152., 155.,\n",
      "         181., 161., 168., 115., 144., 118.]], device='cuda:0')\n",
      "tensor(143.11904907, device='cuda:0') \n",
      "\n",
      "\n",
      "tensor([[    0.00076675,     0.00005007,     0.00032806,     0.00257874,\n",
      "             0.00729370,     0.00946045,     0.00759888,     0.00872803,\n",
      "             0.00781250,     0.00585938,     0.00747681,     0.00994873,\n",
      "             0.00750732,     0.00897217,     0.00933838,     0.01007080,\n",
      "             0.00854492,     0.00842285,     0.01153564,     0.00823975,\n",
      "             0.00769043,     0.00564575,     0.01171875,     0.00927734,\n",
      "             0.00692749,     0.00744629,     0.00729370,     0.01055908,\n",
      "             0.00939941,     0.00823975,     0.00750732,     0.00793457,\n",
      "             0.01190186,     0.01019287,     0.00701904,     0.00772095,\n",
      "             0.01043701,     0.00866699,     0.00848389,     0.00921631,\n",
      "             0.00878906,     0.00747681]], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<MeanBackward1>)\n",
      "tensor(0.00787354, device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sae.use_threshold = True\n",
    "encoded_BLF = sae.encode(activations_BLD)\n",
    "decoded_BLD = sae.decode(encoded_BLF)\n",
    "\n",
    "torch.set_printoptions(precision=8, sci_mode=False)\n",
    "\n",
    "nonzero_BL = einops.reduce((encoded_BLF > 0).float(), \"b l f -> b l\", \"sum\")\n",
    "print(nonzero_BL)\n",
    "mean_nonzero = nonzero_BL.mean()\n",
    "print(mean_nonzero, \"\\n\\n\")\n",
    "\n",
    "MSE_BL = (activations_BLD - decoded_BLD).pow(2).mean(dim=-1)\n",
    "print(MSE_BL)\n",
    "mean_MSE = MSE_BL.mean()\n",
    "print(mean_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.04034185, device='cuda:0')\n",
      "tensor(4.86054754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.03699052, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def reconstruct_activations(model, submodule, sae, inputs_BL):\n",
    "\n",
    "    def gather_target_act_hook(module, inputs, outputs):\n",
    "        # For many models, the submodule outputs are a tuple or a single tensor:\n",
    "        # If \"outputs\" is a tuple, pick the relevant item:\n",
    "        #   e.g. if your layer returns (hidden, something_else), you'd do outputs[0]\n",
    "        # Otherwise just do outputs\n",
    "        if isinstance(outputs, tuple):\n",
    "            activations_BLD = outputs[0]\n",
    "        else:\n",
    "            activations_BLD = outputs\n",
    "\n",
    "        encoded_BLF = sae.encode(activations_BLD)\n",
    "        decoded_BLD = sae.decode(encoded_BLF)\n",
    "\n",
    "        outputs = (decoded_BLD,) + outputs[1:]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    handle = submodule.register_forward_hook(gather_target_act_hook)\n",
    "\n",
    "    try:\n",
    "        outputs = model(input_ids=inputs_BL.to(model.device), labels=inputs_BL.to(model.device))\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during forward pass: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "    return outputs\n",
    "\n",
    "original_loss = model(input_ids=tokens.to(model.device), labels=tokens.to(model.device)).loss\n",
    "\n",
    "outputs = reconstruct_activations(model, submodules[0], sae, tokens)\n",
    "\n",
    "print(outputs.loss)\n",
    "print(original_loss)\n",
    "ratio = outputs.loss / original_loss\n",
    "print(ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
